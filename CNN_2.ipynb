{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ4NwKnI3VaQ"
      },
      "source": [
        "# CNN part 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqLP9gdo3VaS"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, InputLayer\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxEO-LBb3VaW"
      },
      "source": [
        "# CIFAR-10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "    <tbody><tr>\n",
        "        <td class=\"cifar-class-name\">airplane</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">automobile</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">bird</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">cat</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">deer</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">dog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">frog</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">horse</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">ship</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td class=\"cifar-class-name\">truck</td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png\" class=\"cifar-sample\"></td>\n",
        "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png\" class=\"cifar-sample\"></td>\n",
        "    </tr>\n",
        "</tbody></table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-blCKxP3VaX"
      },
      "source": [
        "### Helper functions\n",
        "First, we create a function that will convert __class_id__ (integer from 0 to 9) into proper __name__ we can understand. <br>\n",
        "The mapping between __label__ and __name__ is in order of table above. Airplane = 0, Automobile = 1 ... <br>\n",
        "Our function `get_class_name` will take an integer from 0 to 9 as a input and output string label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JThjqjj63VaY",
        "collapsed": true
      },
      "source": [
        "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "num_classes = len(classes)\n",
        "\n",
        "def get_class_name(class_index):\n",
        "    if class_index < 0 or class_index > 9:\n",
        "        raise ValueError(\"Class Index must be > 0 and <= 9\")\n",
        "    return classes[class_index]\n",
        "\n",
        "print(\"Number of classes\", num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDvINOvu3Vaa"
      },
      "source": [
        "## CIFAR-10 Preprocessing\n",
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jnMJcx-3Vab"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(\"Shape of train images\", x_train.shape, \"containing\", x_train.dtype)\n",
        "print(\"Shape of train labels\", y_train.shape, \"containing\", y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3v0yAho3Vad"
      },
      "source": [
        "### Dataset shape\n",
        "#### Images\n",
        "In case of `train images`, our dataset shape is <b>(50000, 32, 32, 3)</b> <br>\n",
        "This means that we have <b>50000</b> images of size <b>32</b>px by <b>32</b>px. Each image has <b>3</b> channels (RGB). <br>\n",
        "Each value stored in this array is uint8 (unsigned integer ranging from 0 to 255)\n",
        "\n",
        "#### Labels\n",
        "We also have 50000 labels, where each label is an integer from <b>0</b> to <b>9</b>. <br>\n",
        "\n",
        "\n",
        "\n",
        "### Converting labels to one-hot representation\n",
        "y_train[0] <b>before</b> conversion is <b>[6]</b> <br>\n",
        "y_train[0] <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYFY11d3Vae"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ugi2gtQ3Vag"
      },
      "source": [
        "### Normalizing dataset\n",
        "Integer range from 0 to 255 is too large for neural network. <br>\n",
        "Therefore instead of integer, we will use 32 bit float ranging from 0 to 1 <br>\n",
        "Next, we convert training and testing images to float\n",
        "followed by division by 255\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOS8_0W23Vag"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAk-Rw-93Vaj"
      },
      "source": [
        "### Converting from one hot back to label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gup0y4N93Vak"
      },
      "source": [
        "def one_hot_to_label(one_hot):\n",
        "     return np.argmax(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaPLkdtK3Val"
      },
      "source": [
        "## Visualizing dataset\n",
        "\n",
        "For visualization of the data we will use matplotlib.<br>\n",
        "`%matplotlib inline` will enable us to display image directly in Jupyter Notebook. <br>\n",
        "We will display `class id` (label direcly taken from dataset) and converted `name` from function `get_class_name(str)` we created earlier. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5_NHHB63Vam"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "for i in range(2):\n",
        "    label = one_hot_to_label(y_train[i])\n",
        "    plt.figure()\n",
        "    plt.title(\"Class id: \"+ str(label)+ \" and class name: \"+ get_class_name(label))\n",
        "    plt.imshow(x_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75OGaM-j3Van"
      },
      "source": [
        "Convert visualization code above into neat function `visualize(image_arr, class_id)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRd_LLQy3Vao"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def visualize(image_arr, class_id):\n",
        "  for image, label in zip(image_arr, class_id):\n",
        "      plt.figure()\n",
        "      plt.title(\"Class id: \"+ str(label)+ \" and class name: \"+ get_class_name(label))\n",
        "      plt.imshow(image)\n",
        "\n",
        "class_id = [one_hot_to_label(i) for i in y_train[:2]]\n",
        "image_arr = x_train[:2]\n",
        "visualize(image_arr, class_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBtXA-EC3Vaq"
      },
      "source": [
        "### Testing the code\n",
        "Following code is going to use your function to display 5 random images from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuzWBchb3Var"
      },
      "source": [
        "from random import randint\n",
        "for i in range(5):\n",
        "    random_index = randint(0, len(x_train)-1)\n",
        "    visualize(x_train[random_index], y_train[random_index])\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQCwDj_P3Vas"
      },
      "source": [
        "![title](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data//Activation_Functions.png)\n",
        "\n",
        "### List of activation functions provided by default - https://keras.io/activations/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzPfUPws3Vat"
      },
      "source": [
        "Activation('relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ins-ZWY23Vau"
      },
      "source": [
        "### Training parameters\n",
        "<b>Batch</b> size refers to number of samples (images) to proccess at once <br>\n",
        "<b>Epochs</b> indicate the number of times our training will go though the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TESAQPA3Vau"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVpwPoqW3Vav"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUn0Rzhz3Vaw"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXSSYuwM3Vax"
      },
      "source": [
        "### Viewing model details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJU0w07A3Vax"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykXdMTF-3Vaz"
      },
      "source": [
        "### Plotting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "E3fLHWau3Vaz"
      },
      "source": [
        "from IPython.display import Image\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "Image(model_to_dot(model).create(prog='dot', format='png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvNtwH5j3Va1"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v4ioXWw3Va2"
      },
      "source": [
        "Experimenting with different activation functions and observing the behaviour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5W_yWP13Va2"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyiomXdE3Va3"
      },
      "source": [
        "## Model 2\n",
        "### Next we will explore the effect of adding more depth to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkowUuzm3Va3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxOe27_G3Va5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImqsCG053Va6"
      },
      "source": [
        "from IPython.display import Image\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "Image(model_to_dot(model).create(prog='dot', format='png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrQBtWMn3Va7"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPabDJnZ3Va8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR3BqI-j3Va9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Q05Xef3Va9"
      },
      "source": [
        "From validation accuracy after few epochs it might seem that Model 1 is outperforming Model 2.  <br>\n",
        "Due to the added complexity Model 2 is training slightly slower, but has greater potential to outperform Model 1 in the long run.<br>\n",
        "Here is a graph of validation accuracy of both models over 70 epochs.\n",
        "\n",
        "![title](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data//Accuracy_Graph.png)\n",
        "\n",
        "After approximately 7th epoch, Model 2 took the lead and had 10% better validation accuracy at the end of training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kVlbgp3Va-"
      },
      "source": [
        "## Some other commonly used models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNgQWRX13Va_"
      },
      "source": [
        "# State of the Art Models\n",
        "All models presented in this sections are desined for larger images than Cifar10 or Mnist. <br>\n",
        "Minimal dimensions of the input image change from model to model. <br>\n",
        "If we reduce the input dimension of following models to fit Cifar10 dataset (32x32x3), pooling layers within the model will reduce the dimension of data flowing through the network to 0.   \n",
        "Most models in following section will be compatible with ImageNet dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KCtwgBi3Va_"
      },
      "source": [
        "## Inception V3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuonLfL3Va_"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.layers import Input\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "model = InceptionV3(input_tensor=input_tensor, weights=None, include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh0zqcL43VbA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjnM3F6D3VbB"
      },
      "source": [
        "## Inception V3 Graph\n",
        "![title](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data//InceptionV3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVW2DUl3VbB"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbhOUq9B3VbC"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "model = ResNet50(input_tensor=input_tensor, weights=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O3fhHgt3VbD"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz6HdBgG3VbD"
      },
      "source": [
        "# ResNet50 Graph\n",
        "![title](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data//Resnet50.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfJkkYor3VbE"
      },
      "source": [
        "# Model Comparison\n",
        "## ImageNet\n",
        "All following models have been trained and tested on ImageNet dataset, containing over 14 million labeled images.\n",
        "<br><a href=\"http://www.image-net.org/\">http://www.image-net.org/</a>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Model</th>\n",
        "<th align=\"right\">Size</th>\n",
        "<th align=\"right\">Top-1 Accuracy</th>\n",
        "<th align=\"right\">Top-5 Accuracy</th>\n",
        "<th align=\"right\">Parameters</th>\n",
        "<th align=\"right\">Depth</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>Xception</td>\n",
        "<td align=\"right\">88 MB</td>\n",
        "<td align=\"right\">0.790</td>\n",
        "<td align=\"right\">0.945</td>\n",
        "<td align=\"right\">22,910,480</td>\n",
        "<td align=\"right\">126</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>VGG16</td>\n",
        "<td align=\"right\">528 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.901</td>\n",
        "<td align=\"right\">138,357,544</td>\n",
        "<td align=\"right\">23</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>VGG19</td>\n",
        "<td align=\"right\">549 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.900</td>\n",
        "<td align=\"right\">143,667,240</td>\n",
        "<td align=\"right\">26</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ResNet50</td>\n",
        "<td align=\"right\">99 MB</td>\n",
        "<td align=\"right\">0.749</td>\n",
        "<td align=\"right\">0.921</td>\n",
        "<td align=\"right\">25,636,712</td>\n",
        "<td align=\"right\">168</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>InceptionV3</td>\n",
        "<td align=\"right\">92 MB</td>\n",
        "<td align=\"right\">0.779</td>\n",
        "<td align=\"right\">0.937</td>\n",
        "<td align=\"right\">23,851,784</td>\n",
        "<td align=\"right\">159</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>InceptionResNetV2</td>\n",
        "<td align=\"right\">215 MB</td>\n",
        "<td align=\"right\">0.803</td>\n",
        "<td align=\"right\">0.953</td>\n",
        "<td align=\"right\">55,873,736</td>\n",
        "<td align=\"right\">572</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet</td>\n",
        "<td align=\"right\">16 MB</td>\n",
        "<td align=\"right\">0.704</td>\n",
        "<td align=\"right\">0.895</td>\n",
        "<td align=\"right\">4,253,864</td>\n",
        "<td align=\"right\">88</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td align=\"right\">14 MB</td>\n",
        "<td align=\"right\">0.713</td>\n",
        "<td align=\"right\">0.901</td>\n",
        "<td align=\"right\">3,538,984</td>\n",
        "<td align=\"right\">88</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet121</td>\n",
        "<td align=\"right\">33 MB</td>\n",
        "<td align=\"right\">0.750</td>\n",
        "<td align=\"right\">0.923</td>\n",
        "<td align=\"right\">8,062,504</td>\n",
        "<td align=\"right\">121</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet169</td>\n",
        "<td align=\"right\">57 MB</td>\n",
        "<td align=\"right\">0.762</td>\n",
        "<td align=\"right\">0.932</td>\n",
        "<td align=\"right\">14,307,880</td>\n",
        "<td align=\"right\">169</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>DenseNet201</td>\n",
        "<td align=\"right\">80 MB</td>\n",
        "<td align=\"right\">0.773</td>\n",
        "<td align=\"right\">0.936</td>\n",
        "<td align=\"right\">20,242,984</td>\n",
        "<td align=\"right\">201</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>NASNetMobile</td>\n",
        "<td align=\"right\">23 MB</td>\n",
        "<td align=\"right\">0.744</td>\n",
        "<td align=\"right\">0.919</td>\n",
        "<td align=\"right\">5,326,716</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>NASNetLarge</td>\n",
        "<td align=\"right\">343 MB</td>\n",
        "<td align=\"right\">0.825</td>\n",
        "<td align=\"right\">0.960</td>\n",
        "<td align=\"right\">88,949,818</td>\n",
        "<td align=\"right\">-</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.\n",
        "\n",
        "\n",
        "\n",
        "### Using ResNet50!\n",
        "Despite the complexity and increased training time, we can use pre-trained networks to make predictions.\n",
        "\n",
        "#### Obtaining pre-trained weights\n",
        "Keras will allow us to obtain well-trained weights for ResNet50. The training was done on ImageNet dataset, containing over 14 million images. We can simply specify the `weights='imagenet'` while initializing the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8w04Cov3VbE"
      },
      "source": [
        "### Creating pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h93PICu_3VbE"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "model = ResNet50(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCCOzmgL3VbF"
      },
      "source": [
        "### Image to predict\n",
        "Find an image you would like the model to predict and upload it to the notebook folder (folder icon to left). <br>\n",
        "Adjust the `img_path` variable to point to the correct image. <br>\n",
        "It is recommended to start with simple example of an elephant.\n",
        "![elephant](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/elephant.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h4uAawZ3VbF"
      },
      "source": [
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "!wget https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/elephant.jpg\n",
        "img = image.load_img('elephant.jpg', target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# to see more \"less likely\" predictions, adjust the optional top parameter\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqsleMQpth42"
      },
      "source": [
        "# EfficientNet and Transfer Learning\n",
        "### Overview\n",
        "EfficientNet, first introduced in Tan and Le, 2019 is among the most efficient models (i.e. requiring least FLOPS for inference) that reaches State-of-the-Art accuracy on both imagenet and common image classification transfer learning tasks.\n",
        "\n",
        "The following code will show you how EfficientNet works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5pMEsnt2Wr"
      },
      "source": [
        "This model takes input images of shape (224, 224, 3)\n",
        "\n",
        "The input data should range [0, 255]\n",
        "\n",
        "Normalization is included as part of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xPnP7tWt4Fv"
      },
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "model = EfficientNetB0(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnZLRs8t6Kt"
      },
      "source": [
        "### Model we will use\n",
        "Because training EfficientNet on ImageNet takes a **tremendous amount of resources**\n",
        "\n",
        "And several techniques that are not a part of the model architecture itself.\n",
        "\n",
        "Hence the Keras implementation by default loads pre-trained weights obtained via training with AutoAugment.\n",
        "### Model Size\n",
        "For B0 to B7 base models, the input shapes are different. We will use **B0** here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WoFxBkHt5z6"
      },
      "source": [
        "model = EfficientNetB0(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8AnKpwbt9is"
      },
      "source": [
        "### Visualizing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5N-I7UQt-6Q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def format_label(label):\n",
        "    string_label = label_info.int2str(label)\n",
        "    return string_label.split(\"-\")[1]\n",
        "\n",
        "\n",
        "label_info = ds_info.features[\"label\"]\n",
        "for i, (image, label) in enumerate(ds_train.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "    plt.title(\"{}\".format(format_label(label)))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyUOsB9uA5p"
      },
      "source": [
        "### Data Augmentation\n",
        "- We can use preprocessing layers APIs for image augmentation.\n",
        "- This Sequential model object can be used both as a part of the model we later build, and as a function to preprocess data before feeding into the model.\n",
        "- Using them as function makes it easy to visualize the augmented images. Here we plot 9 examples of augmentation result of a given figure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyNZ5gGyuAj4"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "img_augmentation = Sequential(\n",
        "    [\n",
        "        preprocessing.RandomRotation(factor=0.15),\n",
        "        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        preprocessing.RandomFlip(),\n",
        "        preprocessing.RandomContrast(factor=0.1),\n",
        "    ],\n",
        "    name=\"img_augmentation\",\n",
        ")\n",
        "for image, label in ds_train.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        aug_img = img_augmentation(tf.expand_dims(image, axis=0))\n",
        "        plt.imshow(aug_img[0].numpy().astype(\"uint8\"))\n",
        "        plt.title(\"{}\".format(format_label(label)))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAzFbU9HuDxX"
      },
      "source": [
        "### Prepare inputs\n",
        "- Once we verify the input data and augmentation are working correctly, we prepare dataset for training.\n",
        "- The input data are **resized to uniform IMG_SIZE**.\n",
        "- The labels are put into **one-hot** (a.k.a. categorical) encoding. The dataset is batched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iQSQl0SuFM7"
      },
      "source": [
        "# One-hot / categorical encoding\n",
        "def input_preprocess(image, label):\n",
        "    label = tf.one_hot(label, NUM_CLASSES)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    input_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "ds_train = ds_train.batch(batch_size=batch_size, drop_remainder=True)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(input_preprocess)\n",
        "ds_test = ds_test.batch(batch_size=batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeC6E7OxuGvx"
      },
      "source": [
        "# Training from scratch\n",
        "- We will build an EfficientNetB0 with 120 output classes, that is initialized from scratch:\n",
        "> Note: the accuracy will increase very slowly and may overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtPcY_E4uINe"
      },
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = img_augmentation(inputs)\n",
        "outputs = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs = 34  # @param {type: \"slider\", min:10, max:100}\n",
        "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dqQNwruJnv"
      },
      "source": [
        "# Transfer learning from pre-trained weights\n",
        "## Initialize the model with pre-trained ImageNet weights, and we fine-tune it on our own dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On1IwOaiuK95"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "\n",
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = img_augmentation(inputs)\n",
        "    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "\n",
        "    # Freeze the pretrained weights\n",
        "    model.trainable = False\n",
        "\n",
        "    # Rebuild top\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6XRp2RPuMtH"
      },
      "source": [
        "### Training with freeze parameters\n",
        "The first step to transfer learning is to freeze all layers and train only the top layers. For this step, a relatively large learning rate (1e-2) can be used. Note that validation accuracy and loss will usually be better than training accuracy and loss. This is because the regularization is strong, which only suppresses training-time metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naPkS6AsuOCY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qK5T1JKuPXp"
      },
      "source": [
        "model = build_model(num_classes=NUM_CLASSES)\n",
        "\n",
        "epochs = 25  # @param {type: \"slider\", min:8, max:80}\n",
        "hist = model.fit(ds_train, epochs=1, validation_data=ds_test, verbose=1)\n",
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF8zNsQOuQ1F"
      },
      "source": [
        "## Training with unfreezed parameters\n",
        "The second step is to unfreeze a number of layers and fit the model using smaller learning rate. In this example we show unfreezing all layers, but depending on specific dataset it may be desireble to only unfreeze a fraction of all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p0R8UsAuSGi"
      },
      "source": [
        "def unfreeze_model(model):\n",
        "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-20:]:\n",
        "        if not isinstance(layer, layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "unfreeze_model(model)\n",
        "\n",
        "epochs = 10  # @param {type: \"slider\", min:8, max:50}\n",
        "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n",
        "plot_hist(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYNQK45uUPr"
      },
      "source": [
        "# Some reflections\n",
        "- Training from scratch requires very careful choice of hyperparameters and is difficult to find suitable regularization.\n",
        "- It would also be much more demanding in resources.\n",
        "-  Plotting the training and validation accuracy makes it clear that validation accuracy stagnates at a low value."
      ]
    }
  ]
}