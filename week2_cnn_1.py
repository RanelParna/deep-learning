# -*- coding: utf-8 -*-
"""Week2_CNN_1_solutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1309t1aX_gmm_Y2MiAha0enqNqdicabnS

# Convolutional Neural Networks (CNN)

## Basic Imports
"""

import keras
import numpy as np

from numpy.random import seed
import tensorflow as tf

"""## Replicability"""

seed(101)
tf.random.set_seed(101)

"""## Low Level Code

While building model, we will add whole convolution layer in a neat package `Conv2D()`. <br>
But before we do so, it is good to understand the __underlaying mechanics__ and __code implementation__ of convolutions. <br>
In the following example, we will define our image/data array `inputs`, and using `kernel` apply 2D convolution. <br>
Full implemenation of convolution layer can be found <a src="https://github.com/keras-team/keras/blob/8ed57c168f171de7420e9a96f9e305b8236757df/keras/layers/convolutional.py#L161"> here</a>. <br>


A convolution input must have shape of `(BatchSize, width, height, inputChannels)` <br>
A convolution filter must have shape of `(width, height, inputChannels, outputChannels)` <br>

# Convolutions

Convolutional Neural Networks (CNNs) are designed to learn features directly from image pixels. They can classify patterns or objects with extreme variability. Currently, they form the core of various __computer vision systems__ such as Facebooks automated photo tagging, handwritten characters recognition, self-driving cars, marine mammal detection, and medical image analysis. In this lab, we will start by exploring a convolution function which forms the heart of CNNs.

![Convolution](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/convolution_kernal.png)
"""

# We need keras.backend and tensorflow to create proper tensors directly
import keras.backend as K
import tensorflow as tf


inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],
                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],
                      [[11.0], [12.0], [13.0], [14.0], [15.0]],
                      [[16.0], [17.0], [18.0], [19.0], [20.0]],
                      [[21.0], [22.0], [23.0], [24.0], [25.0]]
                     ])


kernel = tf.constant([[1.0,0.0,0.0],
                      [0.0,1.0,0.0],
                      [0.0,0.0,1.0]])



inputs = K.reshape(inputs,(-1,5,5,1))
print("Shape of an Input:", inputs.get_shape)

kernel =K.reshape(kernel,(3,3,1,1))
print("Shape of a Kernel", kernel.get_shape)

strides=(1, 1)
padding='valid' # or you can use 'same'

result = K.conv2d(inputs, kernel, strides=strides, padding=padding)
print("Shape of result:", result.get_shape)

print("Result:", K.eval(result))

"""# Dropout
Dropout consists of __randomly__ setting a fraction `rate` of input units to 0 at each update __during training time__
which helps to __prevent overfitting__. <br>
To balance the overall signal strength, we increase the non-zero outputs accordingly.


## Arguments
`rate`: float between 0 and 1. Fraction of the input units to drop.
"""

inputs = tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
rate = 0.8

result = K.dropout(inputs, rate, None)
print("Result:", K.eval(result))

"""# Pooling

## MaxPooling

Another important concept of CNNs is max-pooling, which is a form of
non-linear down-sampling. Max-pooling partitions the input image into a
set of non-overlapping rectangles and, for each such sub-region, outputs
the maximum value.
Max-pooling is useful in vision for two reasons:
- By eliminating non-maximal values, it reduces computation for upper layers.
- It provides a form of translation invariance.


## AveragePooling
Alternative to MaxPooling is Average pooling, where you take sum of all elements in pool and divide by number of elements.
"""

inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],
                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],
                      [[11.0], [12.0], [13.0], [14.0], [15.0]],
                      [[16.0], [17.0], [18.0], [19.0], [20.0]],
                      [[21.0], [22.0], [23.0], [24.0], [25.0]]
                     ])

inputs = K.reshape(inputs,(-1,5,5,1))
print("Shape of an Input:", inputs.get_shape)

pool_size = (2,2)
strides=(1, 1)
padding='valid'
data_format=None
pool_mode='max' # or use 'avg'

result = K.pool2d(inputs, pool_size=pool_size, strides = strides,
                          padding = padding, data_format = data_format,
                          pool_mode=pool_mode)

print(result.get_shape)
print("Result:", K.eval(result))

inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0]],
                      [[6.0],  [7.0],  [8.0],  [9.0]],
                      [[16.0], [17.0], [18.0], [19.0]],
                      [[21.0], [22.0], [23.0], [24.0]]
                     ])

inputs = K.reshape(inputs,(-1,4,4,1))
print("Shape of an Input:", inputs.shape)

pool_size = (2,2)
strides=(1, 1)
padding='same'
data_format=None
pool_mode='max' # or use 'avg'
# make the same size
result = K.pool2d(inputs, pool_size=pool_size, strides = strides,
                          padding = padding, data_format = data_format,
                          pool_mode=pool_mode)

print(result.shape)

# make half smaller
result = K.pool2d(inputs, pool_size=pool_size, strides = (2,2),
                          padding = padding, data_format = data_format,
                          pool_mode=pool_mode)
print(result.shape)

"""# Flatten

This function will convert tensor with any shape/number of dimensions to tensor of 1 dimension. <br>
Flattening is very commonly used to convert output from convolution layer to dense layer.
"""

inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],
                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],
                      [[11.0], [12.0], [13.0], [14.0], [15.0]],
                      [[16.0], [17.0], [18.0], [19.0], [20.0]],
                      [[21.0], [22.0], [23.0], [24.0], [25.0]]
                     ])

inputs = K.reshape(inputs,(-1,5,5,1))
print("Shape of an Input:", inputs.get_shape)


result = K.batch_flatten(inputs)
print(K.eval(result))

"""# Dataset

We will be building a CNN handwritten digits classifier using the MNIST which is one of the more classical datasets. <br>

We use a pickled version of __MNIST data__ for Python. Use the load method to
load the MNIST data.

- loading the mnist data with Keras Loader

"""

from keras.datasets import mnist
(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = mnist.load_data()

print(X_train_orig.shape)

"""## Visualizing the dataset
We will use __matplot library__ to display an image from the MNIST dataset. <br>
`%matplotlib inline` will allow us to display this image directly in Jupyter Notebook cell. <br>
Since we have __grayscale__ image, we need to specify that, while displaying it via `cmap='gray'`
"""

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline

plt.imshow(X_train_orig[0], cmap='gray')

plt.imshow(X_train_orig[1], cmap='gray')

print(X_train_orig[0].shape)

"""# Shaping dataset

## Images
Currently, the X part of dataset is in form `(number_of_samples, px_width, px_height)` <br>
There is one implied information about the dataset, but we need to directly specify it. This information is regarding number of channels per image. Since MNIST dataset is only greyscale, we need to specify it in the dimensionality of the dataset.
Therefore, we need to convert it from `(60000, 28, 28)` to `(60000,28,28,1)`, where `1` stands for greyscale. <br>
If we had an RGB image, the shape of the dataset would look like this `(60000,28,28,3)`
"""

X_train = X_train_orig.reshape(60000,28,28,1)
X_test = X_test_orig.reshape(10000,28,28,1)

"""## Labels

Label for each image is in form of an __integer__ ranging from 0 to 9. <br>
We can use a __one hot encoding__ to transform them into a __binary matrix__. We know there are 10
classes for this problem, so we can expect the binary matrix to have a width
of 10.

### Converting labels to one-hot representation
y_train_orig[0] <b>before</b> conversion is <b>[5]</b> <br>
y_train_orig[0] <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</b>
"""

print(y_train_orig[0])

from keras.utils import to_categorical
to_categorical(y_train_orig[0], 10)

"""For example, we want to convert `y_train_orig` to **20** class representation"""

to_categorical(y_train_orig[0], 20)
# Error will occur here
# to_categorical(y_train_orig[0], 2)

y_train = to_categorical(y_train_orig)
y_test = to_categorical(y_test_orig)
# Challenge code not here

"""# Building the model
We will now combine previously demonstrated mechanisms into one system. <br>

For this very simple model, we will be using `Conv2D`, `Flatten` and `Dense` layers.


"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten

def create_model():
    model = Sequential()
    model.add(Conv2D(64, kernel_size=3, activation="relu", input_shape=(28,28,1)))
    model.add(Conv2D(32, kernel_size=3, activation="relu"))
    model.add(Flatten())
    model.add(Dense(10, activation="softmax"))

    return model

model = create_model()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)

"""## Data Normalization
Though we can observe that the neural network is learning, the rate is __very slow__ and __learning rate deteriorates very quickly__. <br>
This behaviour is due to extreme differences between `max` (255) and `min` (0) values of our dataset. <br>
Neural networks are performing __best when dataset ranges from 0 to 1__, or in some cases -1 to 1. <br>
Since we can imagine these values as signal strength, very high values, such as 255, are way too overpowering and strengthening non-optimal paths too quickly. <br>
Therefore we need to divide our training and testing dataset by 255 to get values ranging from 0 to 1.

"""

X_train = X_train/255
X_test = X_test/255

"""## Training on Normalized dataset
We will generate a new model and train it on normalized dataset. <br>

"""

model = create_model()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=64)

"""# Saving the model

We can use `model.save(filepath)` to save a Keras model into a single HDF5 file which will contain:

- the architecture of the model, allowing to re-create the model
- the weights of the model
- the training configuration (loss, optimizer)
- the state of the optimizer, allowing to resume training exactly where you left off.

We can then use `keras.models.load_model(filepath)` to re-instantiate your model. `load_model` will also take care of compiling the model using the saved training configuration.
"""

model.save('my_model.h5')

"""## Predicting custom images
We can use the model  created to classify any image; but it is important that to pre-process the image correctly before inputting it into the model. <br>
In following code, we shall take a random (100th) image from testing dataset and classify it. <br>
All inputs to our network have to have same shape structure `(batch_size, width, height, channels)`. <br>
Since we want to predict only one image, our batch_size has to be 1.
"""

from keras.models import load_model

#Get image from testing dataset
test_image = X_test_orig[100]

#Display this image, so we have visual feedback
plt.imshow(test_image, cmap='gray')

#Observe the shape of this image
print(test_image.shape)

#Add 1 empty dimension before pixel data to indicate we have batch of 1
#and add 1 empty dimension after pixel data to indicate we have only 1 channel (greyscale)
test_image = test_image.reshape(1,28,28,1)
print(test_image.shape)

#Normalize data
test_image = test_image/255

model = load_model('my_model.h5')
#Classify selected image using our model
prediction = model.predict(test_image)[0]
print("Raw prediction made by model:", prediction)

#Get the element with highest confidence
most_conf_index = np.argmax(prediction)
answer_confidence = prediction[most_conf_index]

print("Model classified image as", most_conf_index, "with", answer_confidence,"confidence")

"""## Tensorflow Playground

To better visualise the importance of feature maps and feature extraction you can visit the following website and experiment with the structure of the network, hyperparameters to get instant visual feedback and see how your changes reflect the detection of features.

<a href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.77793&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false">Here


![tensorboard](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/Tensorboard.png)
</a>

"""